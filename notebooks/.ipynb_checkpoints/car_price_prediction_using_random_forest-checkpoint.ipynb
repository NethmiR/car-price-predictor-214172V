{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c45e16",
   "metadata": {},
   "source": [
    "# Car Price Prediction Assignment\n",
    "## 1. Problem Definition & Data Collection\n",
    "**Objective:** Predict the price of used cars based on attributes like mileage, year, engine capacity, and model.\n",
    "**Dataset:** A collection of used car listings (Toyota, Suzuki, Honda, etc.) filtered for hatchbacks manufactured after 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a60ded",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.12.1)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/Aca/L4S1/ML/car-price-predictor-214172V/venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc93bf",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preprocessing\n",
    "We load the cleaned dataset which has already undergone:\n",
    "- Removal of units (km, cc)\n",
    "- Filtering for domain (Hatchbacks, Year > 2005, Price <= 10M)\n",
    "- One-Hot Encoding for Fuel Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e6e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_path = 'data/processed/cleaned_car_data.csv'\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"✅ Loaded dataset with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: File not found. Please run 'preprocess.py' first.\")\n",
    "\n",
    "# Overview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf72a89",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "The 'model' column is categorical text (e.g., 'Vitz', 'Swift'). \n",
    "Random Forest requires numerical input, so we use **One-Hot Encoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f671243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Features (X) and Target (y)\n",
    "X = df.drop(columns=['price'], errors='ignore')\n",
    "y = df['price']\n",
    "\n",
    "# One-Hot Encoding for 'model' column\n",
    "# drop_first=True avoids multicollinearity\n",
    "X = pd.get_dummies(X, columns=['model'], drop_first=True)\n",
    "\n",
    "print(f\"Feature set shape after encoding: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc54533f",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "We use **Random Forest Regressor**.\n",
    "\n",
    "**Justification:**\n",
    "- It handles non-linear relationships better than Linear Regression.\n",
    "- It is robust to outliers and doesn't require feature scaling.\n",
    "- It provides built-in feature importance.\n",
    "\n",
    "**Hyperparameters (Pruning):**\n",
    "- `n_estimators=100`: Builds 100 trees for stability.\n",
    "- `max_depth=15`: Limits tree depth to prevent **Overfitting**.\n",
    "- `min_samples_split=5`: Ensures leaves contain enough data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da036e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples:  {X_test.shape[0]}\")\n",
    "\n",
    "# Initialize Model\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,          # Pruning parameter\n",
    "    min_samples_split=5,   # Pruning parameter\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Training Random Forest model...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272fd185",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "We evaluate the model using:\n",
    "- **R² Score:** How well the model explains variance (Accuracy).\n",
    "- **MAE (Mean Absolute Error):** Average error in Rupees.\n",
    "- **RMSE (Root Mean Squared Error):** Penalizes large errors more heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d39a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Model Performance Results:\")\n",
    "print(f\"✅ Accuracy (R² Score): {r2:.4f} ({r2*100:.2f}%)\")\n",
    "print(f\"❌ Mean Absolute Error: Rs. {mae:,.2f}\")\n",
    "print(f\"❌ RMSE:                Rs. {rmse:,.2f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Visualizing Actual vs Predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred, alpha=0.6, color='blue')\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2) # Ideal line\n",
    "plt.xlabel(\"Actual Price (Rs)\")\n",
    "plt.ylabel(\"Predicted Price (Rs)\")\n",
    "plt.title(\"Actual vs Predicted Prices (Random Forest)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb3f865",
   "metadata": {},
   "source": [
    "## 5. Explainability (XAI) - Requirement 4\n",
    "We use **SHAP (SHapley Additive exPlanations)** to explain *why* the model predicts specific prices.\n",
    "\n",
    "- **Summary Plot:** Shows which features are most important globally.\n",
    "- **Dependence Plot:** Shows how a single feature (e.g., Year) affects price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize JS visualization code\n",
    "shap.initjs()\n",
    "\n",
    "# Create TreeExplainer\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# 1. Summary Plot (Feature Importance)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Feature Importance (SHAP Summary)\")\n",
    "shap.summary_plot(shap_values, X_test, show=False)\n",
    "plt.show()\n",
    "\n",
    "# 2. Dependence Plot for 'manufacture_year'\n",
    "# This shows how the Year affects the price prediction\n",
    "print(\"Dependence Plot: Effect of Manufacture Year on Price\")\n",
    "shap.dependence_plot(\"manufacture_year\", shap_values, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
